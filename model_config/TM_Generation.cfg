[Model]
batch_size = 32
test_batch_size = 16
lr = 3e-5
reg = 0
beam_size=1

# -1 / specific lr for encoder
decoder_lr = -1

bert_max_len=384
decoder_num_layers=3
decoder_num_heads=8

demo = False
# monologg/koelectra-small-v3-discriminator // monologg/koelectra-base-v3-discriminator // bert-base-multilingual-cased // monologg/kobert
pretrained_path = monologg/koelectra-base-v3-discriminator
#pretrained_path = google/electra-small-discriminator

# None // saves/TM_Generation/* // saves/TM_Generation/26_20211027-1754
model_weight_path = saves/TM_Generation/29_20211029-1315

# Warmup rate to perform linear learning rate warmup for. (defult = 0.1)
lr_schedule = False
warmup_rate = 0.1

# Mixed precision 사용 여부
mp_enabled = true

# Gradient accumulationn steps
accumulation_steps = 2

# Gradient Clipping
max_grad_norm = 5.0  # diable 하려면 주석 처리

# Stochastic weight averaging
swa_warmup = 4  # disable: -1 or 주석처리